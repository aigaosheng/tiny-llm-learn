{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59758ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CPU LLM Benchmarking: latency vs tokens vs quantization\n",
    "# Author: Sheng + GPT-5\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# === Config ===\n",
    "model_paths = {\n",
    "    \"Q4\": \"/models/llama-2-7b.Q4_K_M.gguf\",\n",
    "    \"Q5\": \"/models/llama-2-7b.Q5_K_M.gguf\",\n",
    "    \"Q8\": \"/models/llama-2-7b.Q8_0.gguf\",\n",
    "}\n",
    "prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Summarize the benefits and risks of AI in healthcare in 100 words.\"\n",
    "]\n",
    "n_threads = 8     # adjust for your CPU cores\n",
    "n_tokens = 64     # tokens to generate\n",
    "results = []\n",
    "\n",
    "# === Benchmark loop ===\n",
    "for quant, path in model_paths.items():\n",
    "    print(f\"\\nLoading model ({quant})...\")\n",
    "    llm = Llama(model_path=path, n_threads=n_threads, n_ctx=2048, verbose=False)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        t0 = time.time()\n",
    "        output = llm(prompt, max_tokens=n_tokens, echo=False)\n",
    "        t1 = time.time()\n",
    "\n",
    "        total_time = t1 - t0\n",
    "        gen_time = output[\"timings\"][\"predicted_ms\"] / 1000 if \"timings\" in output else total_time\n",
    "        tokens_generated = len(output[\"choices\"][0][\"text\"].split())\n",
    "        latency = gen_time / max(1, tokens_generated)\n",
    "\n",
    "        results.append({\n",
    "            \"quant\": quant,\n",
    "            \"prompt_len\": len(prompt.split()),\n",
    "            \"tokens_generated\": tokens_generated,\n",
    "            \"total_time_s\": total_time,\n",
    "            \"latency_s/token\": latency\n",
    "        })\n",
    "        print(f\"{quant} | {len(prompt.split())}w | {tokens_generated}t | {latency:.3f}s/token\")\n",
    "\n",
    "# === Report ===\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== Benchmark Summary ===\")\n",
    "print(df)\n",
    "\n",
    "# Optional: visualize\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for quant in df['quant'].unique():\n",
    "        subset = df[df['quant']==quant]\n",
    "        plt.plot(subset['prompt_len'], subset['latency_s/token'], marker='o', label=quant)\n",
    "    plt.xlabel(\"Prompt length (words)\")\n",
    "    plt.ylabel(\"Latency (s/token)\")\n",
    "    plt.title(\"CPU LLM Benchmark: Latency vs Prompt vs Quant Level\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Install matplotlib for chart visualization (pip install matplotlib).\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
